version: '3'

services:
  # [source_postgres] is a PostgreSQL database that will act as the source database for the ETL process.
  source_postgres:
    image: postgres:latest
    container_name: source_postgres
    ports: 
      - '5433:5432'
    networks:
      - elt_network
    environment:
      - POSTGRES_DB=source_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

    volumes:
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql
      - source_db_data:/var/lib/postgresql/data

# [destination_postgres] is a PostgreSQL database that will act as the destination database for the ETL process.
  destination_postgres: 
    image: postgres:latest
    container_name: destination_postgres
    ports: 
      - '5434:5432'
    networks:
      - elt_network
    environment:
      - POSTGRES_DB=destination_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    # volumes:
    #   - destination_db_data:/var/lib/postgresql/data
    
# [elt_script] is a custom image that contains the Python script that will extract data from the source database, load it into the destination database, and run dbt to transform the data.
  # elt_script:
  #   container_name: elt_script
  #   build:
  #     context: ./elt_script # Directory containing the Dockerfile and elt_script.py
  #     dockerfile: DockerFile # name of the Dockerfile to use for building the image
  #   command: ["./start.sh"]
  #   networks:
  #     - elt_network
  #   depends_on:
  #     - source_postgres
  #     - destination_postgres

# [dbt] is a dbt image that will run the dbt transformation on the data loaded into the destination database.
  # dbt:
  #   container_name: dbt
  #   image: ghcr.io/dbt-labs/dbt-postgres:1.8.2
  #   command: 
  #     [ 
  #       'run', 
  #       '--profiles-dir', '/root',
  #       '--project-dir', '/dbt',
  #     ]
  #   networks:
  #     - elt_network
  #   volumes:
  #     - ./dbt_project:/dbt 
  #     - ~/.dbt:/root 
  #   depends_on:
  #     - elt_script
  #   environment:
  #     DBT_PROFILE: dbt_project # This should match the profile name in profiles.yml
  #     DBT_TARGET: dev
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway" # Required for dbt to connect to the source and destination databases

# [airflow-postgres] is a PostgreSQL database that will be used by Apache Airflow to store metadata and task execution logs.
  airflow-postgres:
    image: postgres:latest
    container_name: airflow-postgres
    networks:
      - elt_network
    environment:
      - POSTGRES_DB=airflow
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
    volumes:
      - airflow-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

# [airflow-init] is a container that initializes the Apache Airflow database by running the airflow db init and airflow users create commands.
  airflow-init:
    image: apache/airflow:latest
    container_name: airflow-init
    depends_on:
      - airflow-postgres
    networks:
      - elt_network
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    command: >
      bash -c "airflow db migrate && airflow users create -u airflow -p airflow -f Abdullah -l Abaza -r Admin -e abdullahabaza96@gmail.com"
  
# [airflow-webserver] is the Apache Airflow webserver that provides a UI for managing and monitoring workflows.
  airflow-webserver:
    build:
      context: .
      dockerfile: DockerFile
    user: root
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt_script:/opt/airflow/elt_script
      - ./dbt_project:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW_CONN_DESTINATION_DB=postgresql+psycopg2://postgres:postgres@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=PyghQJewKcp8LYsVW3cuEiXymn4H1SS9EQnWdKfqsIk=
      - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=airflow
    ports:
      - 8080:8080
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      retries: 5
      start_period: 10s

# [airflow-scheduler] is the Apache Airflow scheduler that schedules and executes workflows.
  airflow-scheduler:
    build:
      context: .
      dockerfile: DockerFile
    user: root
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt_script:/opt/airflow/elt_script
      - ./dbt_project:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXCUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW_CONN_DESTINATION_DB=postgresql+psycopg2://postgres:postgres@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=PyghQJewKcp8LYsVW3cuEiXymn4H1SS9EQnWdKfqsIk=
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=airflow
    command: scheduler



networks:
  elt_network:
    driver: bridge
volumes: 
  source_db_data: {}
  # destination_db_data: {}
  airflow-db-volume: {}



  